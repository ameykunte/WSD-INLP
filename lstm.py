Added lstm # -*- coding: utf-8 -*-
"""utsav_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jQQ96Mq4tJMjdFkRj28JHDCA7kiPzSkb
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim

with open('train.data.txt', 'r') as f:
    data = f.readlines()

# Split data into sentences and labels
sentences = []
labels = []
for line in data:
    line = line.strip().split('\t')
    sentences.append(line[-1])
    labels.append(line[0])

# Tokenize sentences
word_index = {}
index_word = {}
for sentence in sentences:
    for word in sentence.split():
        if word not in word_index:
            index = len(word_index)
            word_index[word] = index
            index_word[index] = word

sequences = []
for sentence in sentences:
    sequence = [word_index[word] for word in sentence.split()]
    sequences.append(sequence)

# Pad sequences to make them of equal length
padded_sequences = []
max_length = max([len(seq) for seq in sequences])
for seq in sequences:
    padded_seq = seq + [0] * (max_length - len(seq))
    padded_sequences.append(padded_seq)

# Create label dictionary and integer labels
label_dict = {}
int_labels = []
counter = 0
for label in set(labels):
    label_dict[label] = counter
    counter += 1
for label in labels:
    int_labels.append(label_dict[label])

# Split data into training and validation sets
split = int(0.8 * len(padded_sequences))
train_sentences = padded_sequences[:split]
train_labels = int_labels[:split]
val_sentences = padded_sequences[split:]
val_labels = int_labels[split:]

# Define LSTM classifier
class LSTMClassifier(nn.Module):
    def __init__(self, vocab_size, embedding_dim, output_size, hidden_size=128):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_size)
        self.fc = nn.Linear(hidden_size, output_size)
    
    def forward(self, inputs):
        embedded = self.embedding(inputs)
        output, _ = self.lstm(embedded)
        logits = self.fc(output[:, -1, :])
        return logits

# Split data into batches
batch_size = 35
n_batches = len(train_sentences) // batch_size
correct = 0
train_sentences_batches = []
train_labels_batches = []

for i in range(n_batches):
    start = i * batch_size
    end = (i + 1) * batch_size
    train_sentences_batches.append(train_sentences[start:end])
    train_labels_batches.append(train_labels[start:end])

# Convert batches to PyTorch tensors
train_sentences_tensors = [torch.LongTensor(batch) for batch in train_sentences_batches]
train_labels_tensors = [torch.LongTensor(batch) for batch in train_labels_batches]

# Train model
model = LSTMClassifier(len(word_index)+1, 128, len(set(labels)))
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.0001)

n_epochs = 10
for epoch in range(n_epochs):
    epoch_loss = 0.0
    for i in range(n_batches):
        optimizer.zero_grad()
        output = model(train_sentences_tensors[i])
        loss = criterion(output, train_labels_tensors[i])
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()
        correct+=1

    print(f"Epoch {epoch+1}/{n_epochs}, Loss: {epoch_loss/n_batches:.4f}")

torch.save(model.state_dict(), 'model.pth')

# Sample test sentence
test_sentence = "I loved the movie! The acting was amazing and the plot was very interesting."

# Convert the sentence to lowercase and split into words
words = test_sentence.lower().split()

# Convert words to integers using the word_index dictionary
word_ints = [word_index[word] for word in words if word in word_index]

# Pad the sequence with zeros to make it the same length as the training sequences
padded_sequence = np.zeros(max_length, dtype=int)
padded_sequence[:len(word_ints)] = word_ints

# Convert the padded sequence to a PyTorch tensor
test_tensor = torch.LongTensor(padded_sequence)

# Reshape the tensor to have a batch size of 1
test_tensor = test_tensor.view(1, -1)

# Make predictions on the test sentence
with torch.no_grad():
    output = model(test_tensor)
    _, predicted = torch.max(output, 1)

# Convert the predicted label to a string
for label, index in label_dict.items():
    if index == predicted:
        predicted_label = label

# Print the predicted label
print("Predicted label:", predicted_label)

# Save the trained model
torch.save(model.state_dict(), 'model.pth')

from tensorflow.keras.preprocessing.sequence import pad_sequences
import torch

def preprocess_test_data(sentences, word_index, max_length):
    sequences = []
    for sentence in sentences:
        sequence = [word_index.get(word, 0) for word in sentence.split()] # use word_index.get() instead of word_index[] to avoid KeyError
        sequences.append(sequence)
    padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')
    return torch.tensor(padded_sequences, dtype=torch.long)

# Load the trained model
model = LSTMClassifier(len(word_index)+1, 128, len(set(labels)))
model.load_state_dict(torch.load('model.pth'))

# Preprocess test data
test_sentences = [line.split('\t')[3] for line in open('test.data.txt').read().strip().split('\n')]
test_labels = [line.split('\t')[1] for line in open('test.data.txt').read().strip().split('\n')]
test_sentences_tensor = preprocess_test_data(test_sentences, word_index, max_length)

# Define a function for preprocessing the test data

# Make predictions using the model
model.eval()
with torch.no_grad():
    output = model(test_sentences_tensor)
_, predicted = torch.max(output, 1)
predicted_labels = [labels[p.item()] for p in predicted]
correct % 100
# Calculate accuracy

total = len(test_labels)
for i in range(total):
    if predicted_labels[i] == test_labels[i]:
        correct += 1
accuracy = correct / total
print(f'Test accuracy: {accuracy:.2f}')

